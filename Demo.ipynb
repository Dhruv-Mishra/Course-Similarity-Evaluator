{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Course Name</th>\n",
       "      <th>Course Code</th>\n",
       "      <th>Course Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>operatingsystems</td>\n",
       "      <td>cse231</td>\n",
       "      <td>Operating system is the interface between the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>operatingsystem</td>\n",
       "      <td>cse231</td>\n",
       "      <td>Operating system is the interface between the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>computerarchitectureandoperatingsystems</td>\n",
       "      <td>cse234</td>\n",
       "      <td>The overall objective of this course is to pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>programmablenetworking</td>\n",
       "      <td>cse567</td>\n",
       "      <td>There has been a massive revolution in network...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>networkadministration</td>\n",
       "      <td>cse233</td>\n",
       "      <td>This course is intended for second year B.Tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>advancedembeddedlogicdesign</td>\n",
       "      <td>ece573</td>\n",
       "      <td>Embedded systems consisting of multi-core proc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>distributedsystems:concepts&amp;design</td>\n",
       "      <td>cse530</td>\n",
       "      <td>This objective of this course is to train stud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>wirelessnetworks</td>\n",
       "      <td>ece538/cse538</td>\n",
       "      <td>This course will cover a variety of mobile sys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>computerorganization</td>\n",
       "      <td>cse112</td>\n",
       "      <td>This course considers a computer as a stand al...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Course Name    Course Code  \\\n",
       "288                         operatingsystems         cse231   \n",
       "46                           operatingsystem         cse231   \n",
       "48   computerarchitectureandoperatingsystems         cse234   \n",
       "293                   programmablenetworking         cse567   \n",
       "47                     networkadministration         cse233   \n",
       "184              advancedembeddedlogicdesign         ece573   \n",
       "72        distributedsystems:concepts&design         cse530   \n",
       "75                          wirelessnetworks  ece538/cse538   \n",
       "33                      computerorganization         cse112   \n",
       "\n",
       "                                    Course Description  \n",
       "288  Operating system is the interface between the ...  \n",
       "46   Operating system is the interface between the ...  \n",
       "48   The overall objective of this course is to pro...  \n",
       "293  There has been a massive revolution in network...  \n",
       "47   This course is intended for second year B.Tech...  \n",
       "184  Embedded systems consisting of multi-core proc...  \n",
       "72   This objective of this course is to train stud...  \n",
       "75   This course will cover a variety of mobile sys...  \n",
       "33   This course considers a computer as a stand al...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import openpyxl\n",
    "import warnings\n",
    "import re\n",
    "import tensorflow_hub as hub\n",
    "import difflib\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import pickle\n",
    "import nltk\n",
    "nltk.download('punkt',quiet=True)\n",
    "nltk.download(\"stopwords\",quiet=True)\n",
    "import string \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "class Course_Loader:\n",
    "    course_names = {}\n",
    "    course_codes = {}\n",
    "    names_list = []\n",
    "    codes_list = []\n",
    "    df = -1\n",
    "    newEntries = False\n",
    "    cosine_sim = -1\n",
    "    indices = -1\n",
    "\n",
    "    def __init__(self):\n",
    "        data = ['Course Name','Course Code','Course Description']\n",
    "        self.df = pd.DataFrame(columns = data)\n",
    "        self.course_codes = {}\n",
    "        self.course_names = {}\n",
    "        self.names_list = []\n",
    "        self.codes_list = []\n",
    "        self.newEntries = False\n",
    "\n",
    "    def getDataset(self):\n",
    "        return self.df\n",
    "    \n",
    "    def addCourse(self, f):\n",
    "        newCourse = Course(f)\n",
    "        self.course_names[newCourse.name] = newCourse\n",
    "        self.course_codes[newCourse.code] = newCourse\n",
    "        new_df = pd.DataFrame({\"Course Name\":[newCourse.name],\"Course Code\":[newCourse.code], \"Course Description\":[newCourse.description]})\n",
    "        self.df = self.df.append(new_df,ignore_index=True)\n",
    "        self.newEntries = True # Keeps track of if new entries were added after calculating the similarity check \n",
    "\n",
    "    def findbyCode(self,code, exact = False):\n",
    "        ans = []\n",
    "        if(not exact):\n",
    "            code = \"\".join(code.lower().split())\n",
    "            out = difflib.get_close_matches(code, self.codes_list, cutoff= 0.0000001, n = 10)\n",
    "            for i in out:\n",
    "                ans.append(self.course_codes[i])\n",
    "        if(code in self.course_codes.keys()):\n",
    "            ans.append(self.course_codes[code])\n",
    "        return ans\n",
    "    \n",
    "    def findbyName(self,name, exact = False):\n",
    "        ans = []\n",
    "        if(not exact):\n",
    "            name = \"\".join(name.lower().split())\n",
    "            out = difflib.get_close_matches(name, self.names_list, cutoff= 0.0000001, n = 10)\n",
    "            for i in out:\n",
    "                ans.append(self.course_names[i])\n",
    "        if(name in self.course_names.keys()):\n",
    "            ans.append(self.course_names[name])\n",
    "        return ans\n",
    "\n",
    "    def initialize_recommender(self):\n",
    "        if(self.newEntries):\n",
    "            self.newEntries = False\n",
    "            self.df = self.df.dropna()\n",
    "            self.df = self.df.drop_duplicates(subset='Course Name', keep='first')\n",
    "            self.df = self.df.reset_index(drop=True)\n",
    "            self.names_list = self.df['Course Name'].unique()\n",
    "            self.codes_list = self.df['Course Code'].unique()\n",
    "            encodings = embed(self.df['Course Description'])\n",
    "            matrix = np.vstack(encodings)\n",
    "            cosine_sim = linear_kernel(matrix, matrix)\n",
    "            indices = pd.Series(self.df.index, index=self.df['Course Name'])\n",
    "            self.cosine_sim = cosine_sim\n",
    "            self.indices = indices\n",
    "\n",
    "    def get_recommendations(self, title):\n",
    "        idx = self.indices[title]\n",
    "        sim_scores = list(enumerate(self.cosine_sim[idx]))\n",
    "        return sim_scores\n",
    "    \n",
    "    def combine_recommendations(self,titles):\n",
    "        out = []\n",
    "        self.initialize_recommender()\n",
    "        for i in range(len(self.df)):\n",
    "            out.append([i,0])\n",
    "        for org_title in titles:            #### Names may or may not be present in the dataset, finds the closest title matching in the dataset\n",
    "            title = difflib.get_close_matches(org_title, self.names_list, cutoff= 0.0000001, n = 1)[0]  #Searching the closest match in the dataset \n",
    "            cur_out = self.get_recommendations(title)\n",
    "            for score in range(len(cur_out)):\n",
    "                out[score][1] += cur_out[score][1]\n",
    "        final_score = sorted(out, key=lambda x: x[1], reverse=True)\n",
    "        df_indices = [i[0] for i in final_score][1:10]\n",
    "        result = self.df.iloc[df_indices]\n",
    "        return result\n",
    "                     \n",
    "class Course:\n",
    "\n",
    "    course_name_list = set([])\n",
    "    file_name = \"\"\n",
    "    code = \"\"\n",
    "    description = \"\"\n",
    "    embedding = []\n",
    "    similarity_matrix = []\n",
    "    file_ptr = \"\"\n",
    "    name = \"\"\n",
    "\n",
    "    def pre_process_text(self, new_s):\n",
    "        new_s = \".\".join(new_s.split(\"\\n\"))\n",
    "        #new_s = \" \".join(new_s.split(\"-\"))\n",
    "        new_s = new_s.lower()\n",
    "        # translate_table = dict((ord(char), \" \") for char in string.punctuation)   \n",
    "        # new_s = new_s.translate(translate_table)\n",
    "        # li = word_tokenize(new_s)\n",
    "        # stop_words = set(stopwords.words(\"english\"))\n",
    "        # filter_li = []\n",
    "        # for words in li:\n",
    "        #     if(words not in stop_words):\n",
    "        #         filter_li.append(words)\n",
    "        # ans = \" \".join(filter_li)\n",
    "        ans = new_s\n",
    "        return ans\n",
    "\n",
    "    def __init__(self, file_pointer):\n",
    "        self.file_name = str(f)\n",
    "        self.file_ptr = file_pointer\n",
    "        self.description = self.find_description() + \" \" + self.find_course_plan()\n",
    "        self.code = self.find_code()\n",
    "        self.name = self.find_name()\n",
    "\n",
    "\n",
    "    def find_description(self):\n",
    "        workbook = openpyxl.load_workbook(self.file_ptr)\n",
    "        worksheet = workbook.active\n",
    "        for coll in range(1,6):\n",
    "            for roww in range(1,20):\n",
    "                if(\"description\" in \"\".join(re.sub(r'[^\\w\\s]','', re.sub(r'\\d+','',str(worksheet.cell(row=roww, column=coll).value).lower())).split())):\n",
    "                    return str(worksheet.cell(row=roww, column=coll+1).value)\n",
    "        return \"-1\"\n",
    "    \n",
    "    def find_code(self):\n",
    "        f = self.file_ptr\n",
    "        workbook = openpyxl.load_workbook(f)\n",
    "        worksheet = workbook.active\n",
    "        for coll in range(1,4):\n",
    "            for roww in range(1,5):\n",
    "                cell_val = \"\".join(\"\".join(str(worksheet.cell(row=roww, column=coll).value).lower().strip().split()).split(\"-\"))\n",
    "                if('code' in cell_val):\n",
    "                    cell_val = \" \".join(\"\".join(str(worksheet.cell(row=roww, column=coll+1).value).lower().strip().split()).split(\"-\"))\n",
    "                    return cell_val\n",
    "        #If not found in the sheet, find in file_name \n",
    "        file_name = str(f).split(\"\\\\\")[-1]\n",
    "        file_name = file_name.split(\"/\")[-1]\n",
    "        file_name = file_name.split(\".\")\n",
    "        if(len(file_name) >= 2):\n",
    "            file_name = file_name[-2]\n",
    "        file_name = \"\".join(file_name.lower().split())\n",
    "        return file_name \n",
    "    \n",
    "    def find_name(self):\n",
    "        f = self.file_ptr\n",
    "        workbook = openpyxl.load_workbook(f)\n",
    "        worksheet = workbook.active\n",
    "        for coll in range(1,4):\n",
    "            for roww in range(1,5):\n",
    "                cell_val = \"\".join(\"\".join(str(worksheet.cell(row=roww, column=coll).value).lower().strip().split()).split(\"-\"))\n",
    "                if('name' in cell_val):\n",
    "                    cell_val = \" \".join(\"\".join(str(worksheet.cell(row=roww, column=coll+1).value).lower().strip().split()).split(\"-\"))\n",
    "                    return cell_val\n",
    "        #If not found in the sheet, find in file_name \n",
    "        file_name = str(f).split(\"\\\\\")[-1]\n",
    "        file_name = file_name.split(\"/\")[-1]\n",
    "        file_name = file_name.split(\".\")\n",
    "        if(len(file_name) >= 2):\n",
    "            file_name = file_name[-2]\n",
    "        file_name = \"\".join(file_name.lower().split())\n",
    "        return file_name\n",
    "    \n",
    "    def find_course_plan(self):\n",
    "        f = self.file_ptr\n",
    "        workbook = openpyxl.load_workbook(f)\n",
    "        worksheet = workbook.active\n",
    "        lec_cell = (-1,-1)\n",
    "        for coll in range(1,20):\n",
    "            for roww in range(1,41):\n",
    "                cell_val = \"\".join(\"\".join(str(worksheet.cell(row=roww, column=coll).value).lower().strip().split()).split(\"-\"))\n",
    "                if(len(cell_val) >= 2 and len(cell_val) <= 30 and 'lecture' in cell_val and 'topic' in cell_val):\n",
    "                    lec_cell = (roww,coll)\n",
    "                    break\n",
    "            if(lec_cell[0]!=-1):\n",
    "                break\n",
    "        if(lec_cell[0] == -1):\n",
    "            return \"-1\"\n",
    "        plan = \"\"\n",
    "        for i in range(8):\n",
    "            new_row = lec_cell[0]+i+1\n",
    "            new_col = lec_cell[1]\n",
    "            cell_val = str(worksheet.cell(row=new_row, column=new_col).value).lower().strip()\n",
    "            plan = plan + \". \" + cell_val\n",
    "        plan = self.pre_process_text(plan)\n",
    "        plan = \". \".join(plan.split(\"•\"))\n",
    "        plan = plan + \".\"\n",
    "        return plan\n",
    "    \n",
    "    def compute_embedding(self):\n",
    "        return embed([self.description])\n",
    "   \n",
    "try:                                              #Loading Course_Loader object if it exists\n",
    "    #print(1/0)\n",
    "    with open(\"Course_Loader_Save\", \"rb\") as f:\n",
    "        c = pickle.load(f)\n",
    "except:                                           #Creating a new object if it doesn't exist and saving it on the disk\n",
    "\n",
    "    path = os.getcwd() # Getting the current directory to access the data files\n",
    "    path = str(path) + \"\\\\Data\\\\\" # Data must be stored inside the Data folder located in the current directory \n",
    "\n",
    "    csv_files = glob.glob(os.path.join(path, \"*.xlsx\")) # Get names of all csv files\n",
    "    \n",
    "    c = Course_Loader()\n",
    "\n",
    "    for f in csv_files:  #Adding files to the course loader object \n",
    "        c.addCourse(f)\n",
    "\n",
    "    with open(\"Course_Loader_Save\", \"wb\") as f:   #Saving the new object on the disk \n",
    "        pickle.dump(c,f)\n",
    "\n",
    "query = [\"operating systems\",\"computer networks\"]\n",
    "c.combine_recommendations(query) #Making a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python37\\lib\\tkinter\\__init__.py\", line 1705, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_7164\\3558949399.py\", line 11, in handle_login\n",
      "    c.combine_recommendations(courses)\n",
      "NameError: name 'c' is not defined\n"
     ]
    }
   ],
   "source": [
    "from tkinter import *\n",
    "from PIL import ImageTk,Image\n",
    "from tkinter import messagebox\n",
    "\n",
    "def handle_login():\n",
    "    email = email_input.get()\n",
    "    password = password_input.get()\n",
    "\n",
    "    courses = email.split()\n",
    "    print(1)\n",
    "    c.combine_recommendations(courses)\n",
    "    #print()\n",
    "\n",
    "root = Tk()\n",
    "\n",
    "root.title('Login Form')\n",
    "#root.iconbitmap('favicon.ico')\n",
    "\n",
    "root.geometry('500x500')\n",
    "\n",
    "root.configure(background='#0096DC')\n",
    "img = Image.open('background.jpg')\n",
    "resized_img = img.resize((70,70))\n",
    "img = ImageTk.PhotoImage(resized_img)\n",
    "\n",
    "img_label = Label(root,image=img)\n",
    "img_label.pack(pady=(10,10))\n",
    "\n",
    "text_label = Label(root,text='Course Similarity Evaluator',fg='white',bg='#0096DC')\n",
    "text_label.pack()\n",
    "text_label.config(font=('verdana',24))\n",
    "\n",
    "email_label = Label(root,text='Enter The Courses(Comma Separated):',fg='white',bg='#0096DC')\n",
    "email_label.pack(pady=(20,5))\n",
    "email_label.config(font=('verdana',12))\n",
    "\n",
    "email_input = Entry(root,width=50)\n",
    "email_input.pack(ipady=6,pady=(1,15))\n",
    "\n",
    "password_label = Label(root,text='Enter Password',fg='white',bg='#0096DC')\n",
    "password_label.pack(pady=(20,5))\n",
    "password_label.config(font=('verdana',12))\n",
    "\n",
    "password_input = Entry(root,width=50)\n",
    "password_input.pack(ipady=6,pady=(1,15))\n",
    "\n",
    "login_btn = Button(root,text='Login Here',bg='white',fg='black',width=20,height=2,command=handle_login)\n",
    "login_btn.pack(pady=(10,20))\n",
    "login_btn.config(font=('verdana',10))\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Course Name            Course Code  \\\n",
      "46                           operatingsystem                 cse231   \n",
      "39                 introductiontoprogramming                 cse101   \n",
      "228                introductionofprogramming                 cse101   \n",
      "0                        advancedprogramming                 cse201   \n",
      "48   computerarchitectureandoperatingsystems                 cse234   \n",
      "287       objectorientedprogramminganddesign        cse600a/ece600a   \n",
      "56                            computervision  cse344/544,ece344/544   \n",
      "44   fundamentalsofdatabasemanagementsystems                 cse202   \n",
      "4                algorithmdesign&analysis(b)                 cse223   \n",
      "\n",
      "                                    Course Description  \n",
      "46   Operating system is the interface between the ...  \n",
      "39   Introduction of Programming is the first progr...  \n",
      "228  Introduction of Programming is the first progr...  \n",
      "0    The Advanced Programming is a successor to the...  \n",
      "48   The overall objective of this course is to pro...  \n",
      "287  It is a 2 credit postgraduate level core cours...  \n",
      "56   This is an introductory course on Computer Vis...  \n",
      "44   This is a first course on database systems at ...  \n",
      "4    This is a follow-up course to DSA (Data Struct...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import openpyxl\n",
    "import warnings\n",
    "import re\n",
    "import tensorflow_hub as hub\n",
    "import difflib\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import pickle\n",
    "from tkinter import *\n",
    "from PIL import ImageTk,Image\n",
    "from tkinter import messagebox\n",
    "\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "class Course_Loader:\n",
    "    course_names = {}\n",
    "    course_codes = {}\n",
    "    names_list = []\n",
    "    codes_list = []\n",
    "    df = -1\n",
    "    newEntries = False\n",
    "    cosine_sim = -1\n",
    "    indices = -1\n",
    "\n",
    "    def __init__(self):\n",
    "        data = ['Course Name','Course Code','Course Description']\n",
    "        self.df = pd.DataFrame(columns = data)\n",
    "        self.course_codes = {}\n",
    "        self.course_names = {}\n",
    "        self.names_list = []\n",
    "        self.codes_list = []\n",
    "        self.newEntries = False\n",
    "\n",
    "    def addCourse(self, f):\n",
    "        newCourse = Course(f)\n",
    "        self.course_names[newCourse.name] = newCourse\n",
    "        self.course_codes[newCourse.code] = newCourse\n",
    "        new_df = pd.DataFrame({\"Course Name\":[newCourse.name],\"Course Code\":[newCourse.code], \"Course Description\":[newCourse.description]})\n",
    "        self.df = self.df.append(new_df,ignore_index=True)\n",
    "        self.newEntries = True # Keeps track of if new entries were added after calculating the similarity check \n",
    "\n",
    "    def findbyCode(self,code, exact = False):\n",
    "        ans = []\n",
    "        if(not exact):\n",
    "            code = \"\".join(code.lower().split())\n",
    "            out = difflib.get_close_matches(code, self.codes_list, cutoff= 0.0000001, n = 10)\n",
    "            for i in out:\n",
    "                ans.append(self.course_codes[i])\n",
    "        if(code in self.course_codes.keys()):\n",
    "            ans.append(self.course_codes[code])\n",
    "        return ans\n",
    "    \n",
    "    def findbyName(self,name, exact = False):\n",
    "        ans = []\n",
    "        if(not exact):\n",
    "            name = \"\".join(name.lower().split())\n",
    "            out = difflib.get_close_matches(name, self.names_list, cutoff= 0.0000001, n = 10)\n",
    "            for i in out:\n",
    "                ans.append(self.course_names[i])\n",
    "        if(name in self.course_names.keys()):\n",
    "            ans.append(self.course_names[name])\n",
    "        return ans\n",
    "\n",
    "    def initialize_recommender(self):\n",
    "        if(self.newEntries):\n",
    "            self.newEntries = False\n",
    "            self.df = self.df.dropna()\n",
    "            self.df = self.df.drop_duplicates(subset='Course Name', keep='first')\n",
    "            self.df = self.df.reset_index(drop=True)\n",
    "            self.names_list = self.df['Course Name'].unique()\n",
    "            self.codes_list = self.df['Course Code'].unique()\n",
    "            encodings = embed(self.df['Course Description'])\n",
    "            matrix = np.vstack(encodings)\n",
    "            cosine_sim = linear_kernel(matrix, matrix)\n",
    "            indices = pd.Series(self.df.index, index=self.df['Course Name'])\n",
    "            self.cosine_sim = cosine_sim\n",
    "            self.indices = indices\n",
    "\n",
    "    def get_recommendations(self, title):\n",
    "        idx = self.indices[title]\n",
    "        sim_scores = list(enumerate(self.cosine_sim[idx]))\n",
    "        return sim_scores\n",
    "    \n",
    "    def combine_recommendations(self,titles):\n",
    "        out = []\n",
    "        self.initialize_recommender()\n",
    "        for i in range(len(self.df)):\n",
    "            out.append([i,0])\n",
    "        for org_title in titles:            #### Names may or may not be present in the dataset, finds the closest title matching in the dataset\n",
    "            title = difflib.get_close_matches(org_title, self.names_list, cutoff= 0.0000001, n = 1)[0]  #Searching the closest match in the dataset \n",
    "            cur_out = self.get_recommendations(title)\n",
    "            for score in range(len(cur_out)):\n",
    "                out[score][1] += cur_out[score][1]\n",
    "        final_score = sorted(out, key=lambda x: x[1], reverse=True)\n",
    "        df_indices = [i[0] for i in final_score][1:10]\n",
    "        result = self.df.iloc[df_indices]\n",
    "        return result\n",
    "                     \n",
    "class Course:\n",
    "    course_name_list = set([])\n",
    "    file_name = \"\"\n",
    "    code = \"\"\n",
    "    description = \"\"\n",
    "    embedding = []\n",
    "    similarity_matrix = []\n",
    "    file_ptr = \"\"\n",
    "    name = \"\"\n",
    "\n",
    "    def __init__(self, file_pointer):\n",
    "        self.file_name = str(f)\n",
    "        self.file_ptr = file_pointer\n",
    "        self.description = self.find_description()\n",
    "        self.code = self.find_code()\n",
    "        self.name = self.find_name()\n",
    "        if(self.description != -1):\n",
    "            self.embedding = self.compute_embedding()\n",
    "\n",
    "    def find_description(self):\n",
    "        workbook = openpyxl.load_workbook(self.file_ptr)\n",
    "        worksheet = workbook.active\n",
    "        description_row = -1\n",
    "        description_col = -1\n",
    "        for coll in range(1,6):\n",
    "            for roww in range(1,20):\n",
    "                if(\"description\" in \"\".join(re.sub(r'[^\\w\\s]','', re.sub(r'\\d+','',str(worksheet.cell(row=roww, column=coll).value).lower())).split())):\n",
    "                    description_row = roww\n",
    "                    description_col = coll\n",
    "                    return str(worksheet.cell(row=roww, column=coll+1).value)\n",
    "        return \"-1\"\n",
    "    \n",
    "    def find_code(self):\n",
    "        f = self.file_ptr\n",
    "        workbook = openpyxl.load_workbook(f)\n",
    "        worksheet = workbook.active\n",
    "        for coll in range(1,4):\n",
    "            for roww in range(1,5):\n",
    "                cell_val = \"\".join(\"\".join(str(worksheet.cell(row=roww, column=coll).value).lower().strip().split()).split(\"-\"))\n",
    "                if('code' in cell_val):\n",
    "                    cell_val = \" \".join(\"\".join(str(worksheet.cell(row=roww, column=coll+1).value).lower().strip().split()).split(\"-\"))\n",
    "                    return cell_val\n",
    "        #If not found in the sheet, find in file_name \n",
    "        file_name = str(f).split(\"\\\\\")[-1]\n",
    "        file_name = file_name.split(\"/\")[-1]\n",
    "        file_name = file_name.split(\".\")\n",
    "        if(len(file_name) >= 2):\n",
    "            file_name = file_name[-2]\n",
    "        file_name = \"\".join(file_name.lower().split())\n",
    "        return file_name \n",
    "    \n",
    "    def find_name(self):\n",
    "        f = self.file_ptr\n",
    "        workbook = openpyxl.load_workbook(f)\n",
    "        worksheet = workbook.active\n",
    "        for coll in range(1,4):\n",
    "            for roww in range(1,5):\n",
    "                cell_val = \"\".join(\"\".join(str(worksheet.cell(row=roww, column=coll).value).lower().strip().split()).split(\"-\"))\n",
    "                if('name' in cell_val):\n",
    "                    cell_val = \" \".join(\"\".join(str(worksheet.cell(row=roww, column=coll+1).value).lower().strip().split()).split(\"-\"))\n",
    "                    return cell_val\n",
    "        #If not found in the sheet, find in file_name \n",
    "        file_name = str(f).split(\"\\\\\")[-1]\n",
    "        file_name = file_name.split(\"/\")[-1]\n",
    "        file_name = file_name.split(\".\")\n",
    "        if(len(file_name) >= 2):\n",
    "            file_name = file_name[-2]\n",
    "        file_name = \"\".join(file_name.lower().split())\n",
    "        return file_name\n",
    "\n",
    "    def compute_embedding(self):\n",
    "        return embed([self.description])\n",
    "   \n",
    "try:                                              #Loading Course_Loader object if it exists\n",
    "    with open(\"Course_Loader_Save\", \"rb\") as f:\n",
    "        c = pickle.load(f)\n",
    "except:                                           #Creating a new object if it doesn't exist and saving it on the disk\n",
    "\n",
    "    path = os.getcwd() # Getting the current directory to access the data files\n",
    "    path = str(path) + \"\\\\Data\\\\\" # Data must be stored inside the Data folder located in the current directory \n",
    "\n",
    "    csv_files = glob.glob(os.path.join(path, \"*.xlsx\")) # Get names of all csv files\n",
    "    \n",
    "    c = Course_Loader()\n",
    "\n",
    "    for f in csv_files:  #Adding files to the course loader object \n",
    "        c.addCourse(f)\n",
    "\n",
    "    with open(\"Course_Loader_Save\", \"wb\") as f:   #Saving the new object on the disk \n",
    "        pickle.dump(c,f)\n",
    "\n",
    "query = [\"operating systems\",\"computer networks\"]\n",
    "c.combine_recommendations(query)    #Making a query\n",
    "\n",
    "def handle_login():\n",
    "    email = email_input.get()\n",
    "    password = password_input.get()\n",
    "\n",
    "    courses = email.split(\",\")\n",
    "    #print(1)\n",
    "    print(c.combine_recommendations(courses))\n",
    "    #print()\n",
    "\n",
    "root = Tk()\n",
    "\n",
    "root.title('Login Form')\n",
    "#root.iconbitmap('favicon.ico')\n",
    "\n",
    "root.geometry('500x500')\n",
    "\n",
    "root.configure(background='#0096DC')\n",
    "img = Image.open('background.jpg')\n",
    "resized_img = img.resize((70,70))\n",
    "img = ImageTk.PhotoImage(resized_img)\n",
    "\n",
    "img_label = Label(root,image=img)\n",
    "img_label.pack(pady=(10,10))\n",
    "\n",
    "text_label = Label(root,text='Course Similarity Evaluator',fg='white',bg='#0096DC')\n",
    "text_label.pack()\n",
    "text_label.config(font=('verdana',24))\n",
    "\n",
    "email_label = Label(root,text='Enter The Courses(Comma Separated):',fg='white',bg='#0096DC')\n",
    "email_label.pack(pady=(20,5))\n",
    "email_label.config(font=('verdana',12))\n",
    "\n",
    "email_input = Entry(root,width=50)\n",
    "email_input.pack(ipady=6,pady=(1,15))\n",
    "\n",
    "password_label = Label(root,text='Enter Password',fg='white',bg='#0096DC')\n",
    "password_label.pack(pady=(20,5))\n",
    "password_label.config(font=('verdana',12))\n",
    "\n",
    "password_input = Entry(root,width=50)\n",
    "password_input.pack(ipady=6,pady=(1,15))\n",
    "\n",
    "login_btn = Button(root,text='Login Here',bg='white',fg='black',width=20,height=2,command=handle_login)\n",
    "login_btn.pack(pady=(10,20))\n",
    "login_btn.config(font=('verdana',10))\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "def generate_similarity_explanation(course1_title, course1_description, course2_title, course2_description):\n",
    "    model_name = \"gpt2\"\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "    prompt = f\"Explain why the following two courses are similar:\\n\\nCourse 1: {course1_title}\\n{course1_description}\\n\\nCourse 2: {course2_title}\\n{course2_description}\\n\\nExplanation: \"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    output = model.generate(input_ids, max_length=200, num_return_sequences=1, no_repeat_ngram_size=2, early_stopping=True)\n",
    "    explanation = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    return explanation\n",
    "\n",
    "# Example course descriptions\n",
    "course1_title = \"Introduction to Data Science\"\n",
    "course1_description = \"This is a course about science\"\n",
    "\n",
    "course2_title = \"Data Science Foundations\"\n",
    "course2_description = \"This is a course about maths\"\n",
    "\n",
    "explanation = generate_similarity_explanation(course1_title, course1_description, course2_title, course2_description)\n",
    "f = open(\"exp.txt\",\"w+\")\n",
    "f.write(explanation)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar points:\n",
      "('This course covers the fundamentals of data science, including data manipulation, visualization, and basic statistical analysis', 'In this course, students will explore the core concepts of data science, including data cleaning, visualization, and basic statistical techniques')\n",
      "\n",
      "Dissimilar points:\n",
      "('This course covers the fundamentals of data science, including data manipulation, visualization, and basic statistical analysis', 'The course will focus on using Python and popular libraries like Pandas and Matplotlib to analyze and visualize data')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def extract_sentences(course_description):\n",
    "    sentences = course_description.split(\".\")\n",
    "    return [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "def get_sentence_embeddings(sentences):\n",
    "    model_name = \"bert-base-uncased\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    embeddings = []\n",
    "    for sentence in sentences:\n",
    "        input_ids = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids)\n",
    "        embeddings.append(output[0][:, 0, :].numpy())\n",
    "\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "def compare_courses(course1_description, course2_description):\n",
    "    course1_sentences = extract_sentences(course1_description)\n",
    "    course2_sentences = extract_sentences(course2_description)\n",
    "\n",
    "    course1_embeddings = get_sentence_embeddings(course1_sentences)\n",
    "    course2_embeddings = get_sentence_embeddings(course2_sentences)\n",
    "\n",
    "    similarity_matrix = cosine_similarity(course1_embeddings, course2_embeddings)\n",
    "\n",
    "    most_similar_indices = np.unravel_index(np.argmax(similarity_matrix), similarity_matrix.shape)\n",
    "    most_dissimilar_indices = np.unravel_index(np.argmin(similarity_matrix), similarity_matrix.shape)\n",
    "\n",
    "    return {\n",
    "        \"similar\": (course1_sentences[most_similar_indices[0]], course2_sentences[most_similar_indices[1]]),\n",
    "        \"dissimilar\": (course1_sentences[most_dissimilar_indices[0]], course2_sentences[most_dissimilar_indices[1]])\n",
    "    }\n",
    "\n",
    "# Example course descriptions\n",
    "course1_description = \"This course covers the fundamentals of data science, including data manipulation, visualization, and basic statistical analysis. Students will learn how to work with various data formats and use Python libraries such as Pandas and Matplotlib.\"\n",
    "course2_description = \"In this course, students will explore the core concepts of data science, including data cleaning, visualization, and basic statistical techniques. The course will focus on using Python and popular libraries like Pandas and Matplotlib to analyze and visualize data.\"\n",
    "\n",
    "comparison = compare_courses(course1_description, course2_description)\n",
    "print(\"Similar points:\")\n",
    "print(comparison[\"similar\"])\n",
    "print(\"\\nDissimilar points:\")\n",
    "print(comparison[\"dissimilar\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys \n",
    "# Load your dataset\n",
    "data = c.df\n",
    "\n",
    "# Combine course names and descriptions\n",
    "data[\"text\"] = data[\"Course Name\"] + \": \" + data[\"Course Description\"]\n",
    "# Save the text data to a file\n",
    "with open(\"training_data.txt\", \"w+\", encoding='utf-8') as f:\n",
    "    for text in data[\"text\"]:\n",
    "        f.write(text + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to C:/Users/Administrator/.cache/huggingface/datasets/text/default-c586b4e8b33a86b8/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 1800.13it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 139.15it/s]\n",
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to C:/Users/Administrator/.cache/huggingface/datasets/text/default-c586b4e8b33a86b8/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 93.68it/s]\n",
      "Using pad_token, but it is not set yet.\n",
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\transformers\\optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "  0%|          | 0/429 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The model did not return a loss from the inputs, only the following keys: logits,past_key_values. For reference, the inputs it received are input_ids,attention_mask.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3156\\1333936028.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m# Fine-tune the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1664\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1665\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1666\u001b[1;33m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1667\u001b[0m         )\n\u001b[0;32m   1668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1927\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1928\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1929\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1931\u001b[0m                 if (\n",
      "\u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2697\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2698\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2699\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2700\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2701\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2743\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"loss\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2744\u001b[0m                 raise ValueError(\n\u001b[1;32m-> 2745\u001b[1;33m                     \u001b[1;34m\"The model did not return a loss from the inputs, only the following keys: \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2746\u001b[0m                     \u001b[1;34mf\"{','.join(outputs.keys())}. For reference, the inputs it received are {','.join(inputs.keys())}.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2747\u001b[0m                 )\n",
      "\u001b[1;31mValueError\u001b[0m: The model did not return a loss from the inputs, only the following keys: logits,past_key_values. For reference, the inputs it received are input_ids,attention_mask."
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"text\", data_files={\"train\": \"training_data.txt\"})\n",
    "train_dataset = dataset[\"train\"]\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "train_dataset = train_dataset.map(lambda x: tokenizer(x[\"text\"], truncation=True, padding=\"max_length\", max_length=128), batched=True)\n",
    "\n",
    "# Load the GPT-Neo model\n",
    "model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "\n",
    "# Set up the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "output is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m         \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/output/resolve/main/vocab.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\transformers\\utils\\hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    428\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m             \u001b[0mlocal_files_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\huggingface_hub\\file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1198\u001b[0m                     \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1199\u001b[1;33m                     \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1200\u001b[0m                 )\n",
      "\u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\huggingface_hub\\file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[0;32m   1540\u001b[0m     )\n\u001b[1;32m-> 1541\u001b[1;33m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1542\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    290\u001b[0m             )\n\u001b[1;32m--> 291\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-645c2398-33a8137b4a8068801cd90777)\n\nRepository Not Found for url: https://huggingface.co/output/resolve/main/vocab.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4644\\2696908267.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mcourse2_description\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"In this course, students will explore the core concepts of data science, including data cleaning, visualization, and basic statistical techniques. The course will focus on using Python and popular libraries like Pandas and Matplotlib to analyze and visualize data.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mexplanation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_similarity_explanation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcourse1_description\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcourse2_description\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexplanation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4644\\2696908267.py\u001b[0m in \u001b[0;36mgenerate_similarity_explanation\u001b[1;34m(course1_description, course2_description)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgenerate_similarity_explanation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcourse1_description\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcourse2_description\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mmodel_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"output\"\u001b[0m  \u001b[1;31m# The output directory from the fine-tuning step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGPTNeoForCausalLM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1783\u001b[0m                     \u001b[0m_raise_exceptions_for_missing_entries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1784\u001b[0m                     \u001b[0m_raise_exceptions_for_connection_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1785\u001b[1;33m                     \u001b[0m_commit_hash\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcommit_hash\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1786\u001b[0m                 )\n\u001b[0;32m   1787\u001b[0m                 \u001b[0mcommit_hash\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_commit_hash\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresolved_vocab_files\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfile_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcommit_hash\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\transformers\\utils\\hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    432\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m         raise EnvironmentError(\n\u001b[1;32m--> 434\u001b[1;33m             \u001b[1;34mf\"{path_or_repo_id} is not a local folder and is not a valid model identifier \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m             \u001b[1;34m\"listed on 'https://huggingface.co/models'\\nIf this is a private repository, make sure to \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m             \u001b[1;34m\"pass a token having permission to this repo with `use_auth_token` or log in with \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: output is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`."
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPTNeoForCausalLM\n",
    "\n",
    "def generate_similarity_explanation(course1_description, course2_description):\n",
    "    model_name = \"output\"  # The output directory from the fine-tuning step\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model = GPTNeoForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    prompt = f\"Explain why the following two courses are similar based on their descriptions:\\n\\nCourse 1: {course1_description}\\n\\nCourse 2: {course2_description}\\n\\nExplanation: \"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    output = model.generate(input_ids, max_length=200, num_return_sequences=1, no_repeat_ngram_size=2, early_stopping=True)\n",
    "    explanation = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    return explanation\n",
    "\n",
    "# Example course descriptions\n",
    "course1_description = \"This course covers the fundamentals of data science, including data manipulation, visualization, and basic statistical analysis. Students will learn how to work with various data formats and use Python libraries such as Pandas and Matplotlib.\"\n",
    "course2_description = \"In this course, students will explore the core concepts of data science, including data cleaning, visualization, and basic statistical techniques. The course will focus on using Python and popular libraries like Pandas and Matplotlib to analyze and visualize data.\"\n",
    "\n",
    "explanation = generate_similarity_explanation(course1_description, course2_description)\n",
    "print(explanation)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
